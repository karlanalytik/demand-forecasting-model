---
title: "Predicting Retail Demand with Machine Learning"
subtitle: "Executive Report"
author: "Diana Treviño / Karla Soto"
jupyter: python3
format: 
  html:
    theme: pulse
    page-layout: full
    toc: true
    toc-title: Contents
    toc-depth: 3
    toc-expand: 2
    toc-location: left
    df-print: kable
---

# 1. Executive Summary

This project develops an end-to-end Machine Learning solution to forecast 
retail sales at a granular product–store–month level. Using three years of 
transactional data from a multi-store retail operation, the model significantly 
improves demand prediction accuracy compared to traditional methods. The 
results demonstrate the potential to reduce inventory imbalances, lower 
operational costs, and enable faster, data-driven decision-making across the 
supply chain.

## 2. Business Context and Problem Statement

The company operates a complex retail network with over 
**22,000 products across 60 locations**. Current demand planning relies on 
manual adjustments and simple statistical methods that fail to capture 
heterogeneous patterns by store, product, and seasonality. This leads to 
chronic overstocking, frequent stockouts, lost revenue, and declining customer 
satisfaction. The business objective is to materially reduce forecasting error 
to support inventory optimization and improve operational efficiency.

## 3. Data Overview

The analysis uses three years of historical daily sales data, comprising 
millions of transactions. Additional datasets provide product, category, and 
store metadata. Data were aggregated to a monthly level to balance signal 
strength and operational relevance. While coverage is broad, demand 
distributions are highly sparse and skewed, reflecting the long-tail nature of 
retail sales.

```{python, setup}
#| include: false

# Libraries
#from plotly.subplots import make_subplots
#import plotly.express as px
import pandas as pd

# Functions

```

```{python, data_prepare}
data = pd.read_feather('data/clean_data.feather')
data.head()
```


## 4. Modeling Approach

An end-to-end data science pipeline was implemented, including data cleaning, 
feature engineering, and supervised learning. Features capture temporal 
dynamics, seasonality, lagged demand, and cross-sectional effects across 
products and stores. Multiple machine learning models were evaluated and 
compared against a naive baseline to ensure measurable performance gains.

## 5. Model Performance and Results

The selected model achieves a substantial reduction in RMSE relative to 
traditional forecasting approaches. Performance is strongest for high-volume 
products but remains stable across most categories and stores. Results 
indicate that the model is capable of learning complex, non-linear demand 
patterns that are not captured by rule-based methods.

## 6. Business Impact Interpretation

Improved forecast accuracy directly translates into better inventory decisions. 
Reducing prediction error enables lower safety stock levels, fewer stockouts, 
and faster replenishment cycles. From a strategic perspective, the model 
supports a shift from reactive planning to proactive, data-driven inventory 
management aligned with margin and service-level objectives.

## 7. Limitations and Risks

The model relies on historical patterns and may underperform during structural 
breaks such as major market disruptions or product launches. Data sparsity for 
low-selling items limits predictive power at the extreme tail. Additionally, 
model outputs require careful integration into operational processes to avoid 
over-automation risks.

## 8. Recommendations and Next Steps

- Deploy the model in a pilot phase for selected stores and product categories.
- Introduce daily or weekly forecast updates with confidence intervals.
- Incorporate external signals such as promotions, holidays, and pricing.
- Invest in MLOps capabilities to monitor performance drift and retrain models 
regularly.

## 9. Appendix

The appendix contains detailed feature definitions, model configurations, 
validation methodology, and supplementary performance metrics. It also 
documents assumptions, data transformations, and reproducibility guidelines for 
future extensions.

